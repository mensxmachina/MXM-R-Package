<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Introduction</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p><br>
<br>
<img src = 'http://bit.ly/mxm_ses_png' width="500" height="1000" align="middle" />
<br>
<br></p>

<h1>Introduction</h1>

<p>The MXM R Package, short for the latin &#39;Mens ex Machina&#39; ( Mind from the Machine ), is a collection of utility functions for feature selection, cross validation and Bayesian Networks. MXM offers many feature selection algorithms focused on providing one or more minimal feature subsets, refered also as variable signatures, that can be used to improve the performance of downstream analysis tasks such as regression and classification, by excluding irrelevant and redundant variables.
In this tutorial we will learn how to use the SES algorithm. For simplicity, we will use a dataset referred as <strong>&ldquo;The Wine Dataset&rdquo;</strong>. </p>

<h1>Loading Data</h1>

<p><strong>The Wine Dataset</strong> contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. Note that the &ldquo;Type&rdquo; variable was transformed into a categorical variable.</p>

<p>So, first of all, for this tutorial analysis, we are loading the &#39;MXM&#39; library and &#39;dplyr&#39; library for handling easier the dataset. </p>

<pre><code class="r">### ~ ~ ~ Load Packages ~ ~ ~ ###
library(MXM) 
library(dplyr)
</code></pre>

<p>And on a next step we are downloading and opening the dataset, defining also the column names.</p>

<pre><code class="r">### ~ ~ ~ Load The Dataset ~ ~ ~ ###
wine.url &lt;- &quot;ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine/wine.data&quot;
wine &lt;- read.csv(wine.url,
                 check.names = FALSE,
                 header = FALSE) 
head(wine)
</code></pre>

<pre><code>##   V1    V2   V3   V4   V5  V6   V7   V8   V9  V10  V11  V12  V13  V14
## 1  1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065
## 2  1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050
## 3  1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185
## 4  1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480
## 5  1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93  735
## 6  1 14.20 1.76 2.45 15.2 112 3.27 3.39 0.34 1.97 6.75 1.05 2.85 1450
</code></pre>

<pre><code class="r">str(wine)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    178 obs. of  14 variables:
##  $ V1 : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ V2 : num  14.2 13.2 13.2 14.4 13.2 ...
##  $ V3 : num  1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 ...
##  $ V4 : num  2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ...
##  $ V5 : num  15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ...
##  $ V6 : int  127 100 101 113 118 112 96 121 97 98 ...
##  $ V7 : num  2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ...
##  $ V8 : num  3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 ...
##  $ V9 : num  0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ...
##  $ V10: num  2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 ...
##  $ V11: num  5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ...
##  $ V12: num  1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 ...
##  $ V13: num  3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ...
##  $ V14: int  1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ...
</code></pre>

<pre><code class="r">colnames(wine) &lt;- c(&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, 
                    &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, 
                    &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;,
                    &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, 
                    &#39;Dilution&#39;, &#39;Proline&#39;)
</code></pre>

<h1>SES for Continuous</h1>

<p>For this tutorial example, we are going to apply the SES algorithm on the above dataset, using as data and as target variables only continuous variables.   </p>

<h2>Running SES for 1st time</h2>

<h3>Selecting Appropriate Conditional Independence Test</h3>

<p>The selection of the appropriate conditional independence test is a crucial decision for the validity and success of downstream statistical analysis and machine learning tasks. Currently the __ <code>MXM R package</code>__  supports numerous tests for different combinations of <strong>target</strong> ( <em>dependent</em> ) and <strong>predictor</strong> ( <em>independent</em> ) variables. A detailed summary table to guide you through the selection of the most suitable test can be found in <strong>MXM&#39;s</strong> reference manual (p.21 <em>&ldquo;CondInditional independence tests&rdquo;</em> ) here: <a href="https://CRAN.R-project.org/package=MXM">https://CRAN.R-project.org/package=MXM</a>. 
In our example we will use the <strong><code>MXMX::SES()</code></strong>, which is the implementation of the SES algorithm and since we are going to examine only continuous variables, we will use the Fisher&#39;s Independence Test.</p>

<h3>Creating Data &amp; Target Matrices</h3>

<p><code>dataset</code> - A numeric matrix ( or a <em>data.frame</em> in case of categorical predictors), 
containing the variables for performing the test. The rows should refer to the different samples and columns to the features. For the purposes of this example analysis, we are going to use only the continuous variables, therefore we are removing the &ldquo;Type&rdquo; variable from the dataset. Furthermore, we are removing the &ldquo;Nonflavanoids&rdquo; variable, because we will use it as target.</p>

<pre><code class="r">### ~ ~ ~ Removing The Categorical (&#39;Type&#39;) and The Target (&#39;Nonflavanoids&#39;) Variables ~ ~ ~ ###

wine_dataset &lt;- dplyr::select(wine,
                              -contains(&quot;Type&quot;),
                              -contains(&quot;Nonflavanoids&quot;)) 
head(wine_dataset)
</code></pre>

<pre><code>##   Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids
## 1   14.23  1.71 2.43       15.6       127    2.80       3.06
## 2   13.20  1.78 2.14       11.2       100    2.65       2.76
## 3   13.16  2.36 2.67       18.6       101    2.80       3.24
## 4   14.37  1.95 2.50       16.8       113    3.85       3.49
## 5   13.24  2.59 2.87       21.0       118    2.80       2.69
## 6   14.20  1.76 2.45       15.2       112    3.27       3.39
##   Proanthocyanins Color  Hue Dilution Proline
## 1            2.29  5.64 1.04     3.92    1065
## 2            1.28  4.38 1.05     3.40    1050
## 3            2.81  5.68 1.03     3.17    1185
## 4            2.18  7.80 0.86     3.45    1480
## 5            1.82  4.32 1.04     2.93     735
## 6            1.97  6.75 1.05     2.85    1450
</code></pre>

<p><code>target</code> -  The class variable including the values of the target variable. We should provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object.
For the purposes of this example analysis, we are going to use as the dependent variable Nonflavanoids. </p>

<pre><code class="r">wine_target &lt;- wine$Nonflavanoids
head(wine_target)
</code></pre>

<pre><code>## [1] 0.28 0.26 0.30 0.24 0.39 0.34
</code></pre>

<h3>Function&#39;s Arguments</h3>

<p>This is the first time that we are running the algorithm, so we are going to explain what each <strong>Argument</strong> refers to:</p>

<p><code>target</code> : The class variable. Provide either a string, an integer, a numeric value, a vector, a factor, an ordered factor or a Surv object. As explained above, this will be the dependent variable. If the target is a single integer value or a string, it has to corresponds to the column number or to the name of the target feature in the dataset. <em>Here</em> we choose wine$Nonflavanoids</p>

<p><code>dataset</code> : The dataset. Provide either a data frame or a matrix. If the dataset (predictor variables) contains missing (NA) values, they will automatically be replaced by the current variable (column) mean value with an appropriate warning to the user after the execution. <em>Here</em> we choose the whole wine dataset, except from the Type (categorical) and Nonflavanoids (target) variables.</p>

<p><code>max_k</code> : The maximum conditioning set to use in the conditional independence test (see The Rule of Thumb in <strong>TIPS</strong> session). Integer, default value is 3.<em>Here</em> we choose the default value 3.</p>

<p><code>threshold</code> : Threshold (suitable values in [0,1]) for assessing p-values significance. Default value is 0.05. <em>Here</em> we choose the default value (0.05)</p>

<p><code>test</code> : The conditional independence test to use. Default value is NULL. <em>Here</em> since our dataset includes only continuous features (<em>remember</em>: Categorical variable &#39;Type&#39; was removed) and our dependent variable is also continuous, we choose &#39;testIndFisher&#39;.</p>

<p>For more information, about which test to use, please visit :  <a href="https://www.rdocumentation.org/packages/MXM/versions/0.9.7/topics/CondInditional%20independence%20tests">https://www.rdocumentation.org/packages/MXM/versions/0.9.7/topics/CondInditional%20independence%20tests</a>. </p>

<p><code>ini</code> : After running SES with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subsequent runs of course) by 50%. <em>Here</em>, since we are going to apply the analysis for the first time, we place the argument being equal to <strong>NULL</strong>.</p>

<p><code>wei</code> : A vector of weights to be used for weighted regression. The default value is NULL. It is not taken into account when robust is set to TRUE. <em>Here</em> we are not going to use weights.</p>

<p><code>user_test</code> : A user-defined conditional independence test (provide a closure type object). Default value is NULL. If this is defined, the &ldquo;test&rdquo; argument is ignored. <em>Here</em> we have not created a different conditional independence test. We are going to use the &#39;testIndFisher&#39;, as explained in the <code>test</code> argument.</p>

<p><code>hash</code> : A boolean variable which indicates whether (TRUE) or not (FALSE) to store the statistics calculated during SES execution in a hash-type object. Default value is FALSE. If TRUE a hashObject is produced. In this example, this is the first time that we are running the algorithm and our goal is to apply also a second one. <em>Here</em> mind that we are setting <code>hash</code> = <strong>TRUE</strong>, because we want that the univariate associations to be kept for next usage.</p>

<p><code>hashObject</code> : A List with the hash objects generated in a previous run of SES. Each time SES runs with &ldquo;hash=TRUE&rdquo; it produces a list of hashObjects that can be re-used in order to speed up next runs of SES or MMPC. <em>Here</em>, since this is the first time that we run the analysis, we are not providing a hashObject.</p>

<p><code>ncores</code> :  How many cores to use. For more details, see the <strong>TIPS</strong> session. <em>Here</em>, since we have a small number of samples and features, we are using only one core.</p>

<pre><code class="r">### ~ ~ ~ Running SES For First Time ~ ~ ~ ###
ses_default_1st &lt;- MXM::SES(target = wine_target,
                          dataset   = wine_dataset, 
                          max_k      = 3, 
                          threshold  = 0.05, 
                          test       = &quot;testIndFisher&quot;,
                          ini        = NULL, 
                          wei        = NULL,
                          user_test  = NULL, 
                          hash       = TRUE, 
                          hashObject = NULL, 
                          ncores     = 1)
</code></pre>

<p>So, the algorithm run&hellip;
Let&#39;s see what information we can take out of it.  </p>

<h3>Output</h3>

<p>The main purpose of running SES algorithm is to see which variables should be selected as important. The indices of those variables are stored in <code>selectedVars</code>.  </p>

<pre><code class="r">ses_default_1st@selectedVars
</code></pre>

<pre><code>## [1]  4  5  7 11
</code></pre>

<pre><code class="r">SelectedVars_names&lt;-colnames(wine_dataset[ses_default_1st@selectedVars])
SelectedVars_names
</code></pre>

<pre><code>## [1] &quot;Alcalinity&quot; &quot;Magnesium&quot;  &quot;Flavanoids&quot; &quot;Dilution&quot;
</code></pre>

<p>Here, we see which are the important features.</p>

<p>But can we have them in an order, according to increasing p-value?</p>

<p>Yes, of course!  </p>

<pre><code class="r">SelectedVars_indecies_Ordered&lt;- ses_default_1st@selectedVarsOrder
SelectedVars_indecies_Ordered
</code></pre>

<pre><code>## [1]  4  5  7 11
</code></pre>

<p>And what are the equivalent signatures?</p>

<pre><code class="r">ses_default_1st@signatures
</code></pre>

<pre><code>##      Var1 Var2 Var3 Var4
## [1,]    4    5    7   11
</code></pre>

<p>But what is the difference of signatures and this:</p>

<pre><code class="r">ses_default_1st@queues
</code></pre>

<pre><code>## [[1]]
## [1] 4
## 
## [[2]]
## [1] 5
## 
## [[3]]
## [1] 7
## 
## [[4]]
## [1] 11
</code></pre>

<p>Well, queues is a list containing a list (queue) of equivalent features for each variable included in selectedVars. An equivalent signature can be built by selecting one and only one feature from each queue. In our case there are no equivalent signatures, hence all variables have no equivalent variable.  </p>

<p>What about the statistics corresponding to &ldquo;p-values&rdquo;? (Remember: higher values indicates higher association)</p>

<pre><code class="r">ses_default_1st@stats  
</code></pre>

<pre><code>##  [1] 0.4599480 1.2376687 0.4068903 2.9376490 2.4364353 0.4707855 2.2854487
##  [8] 0.3049473 1.8515479 0.5539026 2.0418569 0.8210702
</code></pre>

<p>But what is the difference between @stats and @pvalues?</p>

<pre><code class="r">ses_default_1st@pvalues
</code></pre>

<pre><code>##  [1] -0.4367580 -1.5255269 -0.3789372 -5.5846020 -4.1451887 -0.4488160
##  [7] -3.7504527 -0.2734232 -2.7215042 -0.5441125 -3.1541338 -0.8849622
</code></pre>

<p>Both lists are almost the same, however, in @pvalues, for each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate higher association.</p>

<p>And are all hyper-parameters that were used for the algorithm also stored in this Object?  </p>

<pre><code class="r">ses_default_1st@max_k     # max_k option used in the current run
</code></pre>

<pre><code>## [1] 3
</code></pre>

<pre><code class="r">ses_default_1st@threshold # threshold option used in the current run
</code></pre>

<pre><code>## [1] -2.995732
</code></pre>

<pre><code class="r">ses_default_1st@test      # character name of the statistic test used
</code></pre>

<pre><code>## [1] &quot;testIndFisher&quot;
</code></pre>

<p>Ok! All this is perfect, but can we see the number of univariate associations?</p>

<pre><code class="r">ses_default_1st@n.tests 
</code></pre>

<pre><code>## [1] 55
</code></pre>

<p><strong>Attention</strong> : If you have set <code>hash = TRUE</code> (as we did), then the number of tests performed by SES will be returned. So actually here we are seeing the number of tests performed.</p>

<p>Ok, but in the beginning we said that if we enable <code>hash</code>, then the univariate associations are also stored. How can we get them?</p>

<pre><code class="r">ses_default_1st@univ
</code></pre>

<pre><code>## $stat
##  [1] -2.079719  3.992701  2.492686  5.014962 -3.467758 -6.410909 -7.953067
##  [8] -5.074785  1.851548 -3.557762 -7.324434 -4.260699
## 
## $pvalue
##  [1]  -3.243966  -9.251082  -4.297126 -13.558111  -7.322485 -20.461002
##  [7] -29.158820 -13.831160  -2.721504  -7.638713 -25.499442 -10.312081
</code></pre>

<p>And how quick has all this happened? </p>

<pre><code class="r">ses_default_1st@runtime 
</code></pre>

<pre><code>##    user  system elapsed 
##    0.05    0.00    0.05
</code></pre>

<h2>Running SES 2nd time</h2>

<p>As mentioned before, we want to apply the analysis more than once. So, let&#39;s say that the goal is to apply the same analysis, but with a different threshold (0.1 instead of 0.05) and with a different max_k (4 instead of 3).
Remember that during the first run, we kept <code>hash = TRUE</code>, therefore the information is stored in the created variable. <code>ini</code>  is a supposed to be a list. After running SES (or MMPC) with some hyper-parameters you might want to run SES again with different hyper-parameters. To avoid calculating the univariate associations (first step of SES) again, you can extract them from the first run of SES and plug them here. This can speed up the second run (and subsequent runs of course) by 50%. </p>

<p>In addition, supplying the hashObject also saves computational time as no extra tests will be performed.</p>

<pre><code class="r">### ~ ~ ~ Running SES For First Time ~ ~ ~ ###
ses_default_2nd &lt;- MXM::SES(target = wine_target,
                          dataset   = wine_dataset, 
                          max_k      = 4, 
                          threshold  = 0.1, 
                          test       = &quot;testIndFisher&quot;,
                          ini        = ses_default_1st@univ, 
                          wei        = NULL,
                          user_test  = NULL, 
                          hash       = TRUE, 
                          hashObject = ses_default_1st@hashObject, 
                          ncores     = 1)
</code></pre>

<h2>Creating the model</h2>

<p>This is how we can create the Model including the significant variables. One or more regression models obtained from SES are returned.</p>

<pre><code class="r">### ~ ~ ~ glm() Model Estimates Using SES Feature Subset As Predictor Variables ~ ~ ~ ###

first_sign_ses&lt;- ses.model(target = wine_target, 
                             dataset = as.matrix(wine_dataset),
                             wei = NULL, 
                             sesObject = ses_default_1st, # 
                             test = &#39;testIndFisher&#39;)
</code></pre>

<p>The model(s) created is/are stored in the \(mod\) variable. We can get the summary of the model (looks like the summary of many other linear models), or ask for the signatures ( \(signature\) ). The BIC score is also returned for each value of the signature. </p>

<pre><code class="r">ses_model_summary &lt;- first_sign_ses$mod
signature          &lt;- first_sign_ses$signature
signature
</code></pre>

<pre><code>## Alcalinity  Magnesium Flavanoids   Dilution        bic 
##     4.0000     5.0000     7.0000    11.0000  -288.7666
</code></pre>

<h1>SES for Categorical</h1>

<p>On the example above, we run the analysis for a continuous variable (Nonflavanoids). What would happen if we choose to use as target the &ldquo;Type&rdquo; variable, which is the categorical variable referring to the three types of wine that are represented?</p>

<h2>Running SES for Categorical</h2>

<h3>Selecting Appropriate Conditional Independence Test</h3>

<p>Since the variable is categorical - and more specific it is a factor with more than two levels (unordered) -and the features are continuous, according to <strong>MXM&#39;s</strong> reference manual (p.21 <em>&ldquo;CondInditional independence tests&rdquo;</em> ) here: <a href="https://CRAN.R-project.org/package=MXM">https://CRAN.R-project.org/package=MXM</a>, we should use the Multinomial logistic regression ( &#39;testIndMultinom&#39; ).</p>

<h3>Creating Data &amp; Target Matrices</h3>

<p>In this step, we keep the whole dataset, in order to show how to use the algorithm also without subtracting the initial matrix.</p>

<pre><code class="r">### ~ ~ ~ Taking The Whole Dataset ~ ~ ~ ###

wine_dataset &lt;- wine
head(wine_dataset)
</code></pre>

<pre><code>##   Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids
## 1    1   14.23  1.71 2.43       15.6       127    2.80       3.06
## 2    1   13.20  1.78 2.14       11.2       100    2.65       2.76
## 3    1   13.16  2.36 2.67       18.6       101    2.80       3.24
## 4    1   14.37  1.95 2.50       16.8       113    3.85       3.49
## 5    1   13.24  2.59 2.87       21.0       118    2.80       2.69
## 6    1   14.20  1.76 2.45       15.2       112    3.27       3.39
##   Nonflavanoids Proanthocyanins Color  Hue Dilution Proline
## 1          0.28            2.29  5.64 1.04     3.92    1065
## 2          0.26            1.28  4.38 1.05     3.40    1050
## 3          0.30            2.81  5.68 1.03     3.17    1185
## 4          0.24            2.18  7.80 0.86     3.45    1480
## 5          0.39            1.82  4.32 1.04     2.93     735
## 6          0.34            1.97  6.75 1.05     2.85    1450
</code></pre>

<p>We will not create a different matrix for the target. As mentioned above, we are going to use the Type variable, but please&hellip; be patient&hellip;</p>

<h3>Setting the Arguments</h3>

<pre><code class="r">### ~ ~ ~ Running SES For Categorical Variable ~ ~ ~ ###
wine[, 1] &lt;- as.factor(wine[, 1])
ses_default_1st &lt;- MXM::SES(target = 1, ## Defining as target the 1st column
                          dataset   = wine, 
                          max_k      = 3, 
                          threshold  = 0.05, 
                          test       = &quot;testIndMultinom&quot;,
                          ini        = NULL, 
                          wei        = NULL,
                          user_test  = NULL, 
                          hash       = TRUE, 
                          hashObject = NULL, 
                          ncores     = 1)
</code></pre>

<p>So, the algorithm run once again&hellip;
Let&#39;s see what information we can take out of it.</p>

<h3>Output</h3>

<p>The main purpose of running SES algorithm is to see which variables should be selected as  important. The indices of those variables are stored in <code>selectedVars</code>.</p>

<pre><code class="r">ses_default_1st@selectedVars
</code></pre>

<pre><code>## [1]  2  4  8 11 13 14
</code></pre>

<pre><code class="r">SelectedVars_names&lt;-colnames(wine_dataset[ses_default_1st@selectedVars])
SelectedVars_names
</code></pre>

<pre><code>## [1] &quot;Alcohol&quot;    &quot;Ash&quot;        &quot;Flavanoids&quot; &quot;Color&quot;      &quot;Dilution&quot;  
## [6] &quot;Proline&quot;
</code></pre>

<p>We could again ask for each variable of the output, as in the first implementation of the algorithm. But we will concentrate on the returned signature.  </p>

<pre><code class="r">SelectedVars_indecies_Ordered&lt;- ses_default_1st@selectedVarsOrder
SelectedVars_indecies_Ordered
</code></pre>

<pre><code>## [1]  8 14 11  2 13  4
</code></pre>

<p>And what are the equivalent signatures?</p>

<pre><code class="r">ses_default_1st@signatures
</code></pre>

<pre><code>##      Var1 Var2 Var3 Var4 Var5 Var6
## [1,]    2    4    8   11   13   14
</code></pre>

<p>And how quick has all this happened? </p>

<pre><code class="r">ses_default_1st@runtime 
</code></pre>

<pre><code>##    user  system elapsed 
##    4.14    0.00    4.44
</code></pre>

<h2>Creating the model</h2>

<p>This is how we can create the Model including the significant variables. One or more regression models obtained from SES are returned.</p>

<pre><code class="r">### ~ ~ ~ glm() Model Estimates Using SES Feature Subset As Predictor Variables ~ ~ ~ ###

second_sign_ses&lt;- ses.model(target = as.matrix(wine_target), 
                             dataset = as.matrix(wine_dataset),
                             wei = NULL, 
                             sesObject = ses_default_2nd, # 
                             test = &#39;testIndFisher&#39;)
</code></pre>

<p>The model(s) created is/are stored in the \(mod\) variable. We can get the summary of the model (looks like the summary of many other linear models), or ask for the signatures ( \(signature\) ). The BIC score is also returned for each value of the signature. </p>

<pre><code class="r">ses_model_summary &lt;- second_sign_ses$mod
signature          &lt;- second_sign_ses$signature
signature
</code></pre>

<pre><code>##        Ash Alcalinity    Phenols      Color        bic 
##      4.000      5.000      7.000     11.000   -265.697
</code></pre>

<pre><code class="r">str(signature)
</code></pre>

<pre><code>##  Named num [1:5] 4 5 7 11 -266
##  - attr(*, &quot;names&quot;)= chr [1:5] &quot;Ash&quot; &quot;Alcalinity&quot; &quot;Phenols&quot; &quot;Color&quot; ...
</code></pre>

<h1>Permutation Based SES</h1>

<p>If the analysis has to be done using permutations (when few observations are available), then this could be applied like this:</p>

<pre><code class="r">### ~ ~ ~ Permutations ~ ~ ~ ###
library(&#39;MXM&#39;)
permutation_ses_model &lt;- MXM::perm.ses(wine_target,
                                         wine_dataset,  
                                         R = 999, # The number of permutations to use. The default value is 999
                                         max_k      = 3, 
                                         threshold  = 0.05, 
                                         test       = NULL, 
                                         ini        = NULL, 
                                         wei        = NULL,
                                         user_test  = NULL, 
                                         hash       = FALSE, 
                                         hashObject = NULL, 
                                         ncores     = 1)
</code></pre>

<p>There is a solution to avoid all permutations. As soon as the number of times the permuted test statistic
is more than the observed test statistic is more than 50 (if threshold = 0.05 and R = 999), the p-value has exceeded the significance level (threshold value) and hence the predictor variable is not significant. There is no need to continue doing the extra permutations, as a decision has already been made.   </p>

<h1>Cross - Validation</h1>

<p>Cross-validation (CV) is a technique for validating models which can assess the generalization of the results provided by a statistical analysis on independent datasets. It is a technique widely used in regression and classification approaches and provides a solution to the problem of choosing the right tuning-parameters in a prediction model. In the case of SES algorithm, the tuning parameters would be <code>max_k</code> and <code>threshold</code>. Therefore, in MXM package, a function named <code>cv.ses()</code> can be found. The function performs a k-fold cross-validation for identifying the best values for the SES tuning parameters. In case the user does not know what tuning parameters to use, we suggest the use of this function as Step Zero, before running the algorithm.</p>

<h2>Creating Data &amp; Target Matrices</h2>

<p>For the purposes of this tutorial, we are going to use the same dataset and as target we will use again the Categorical variable &ldquo;Type&rdquo;, transforming our problem into a Classification problem by trying to predict the label of the each row, provided by the first column.</p>

<p>So, let&#39;s see how this algorithm works.  </p>

<h2>Function&#39;s Arguments</h2>

<p>Before running the <code>cv.ses()</code> function we are going to explain what each <strong>Argument</strong> refers to:  </p>

<p><code>target</code> : The target or class variable as in SES. The difference is that it cannot accept a single numeric value, an integer indicating the column in the dataset. <em>Here</em> we choose wine$Type, which is a Categorical variable.</p>

<p><code>dataset</code> : The dataset object as in SES. <em>Here</em> we choose the whole wine dataset.</p>

<p><code>wei</code> : A vector of weights to be used for weighted regression.<em>Here</em> we choose the default value <strong>NULL</strong>.  </p>

<p><code>kfolds</code> : The number of the folds in the k-fold Cross Validation (integer). <em>Here</em> we choose 5. Since the dataset includes 178 observations, by splitting into 5 folds, each fold will have circa 35 rows.</p>

<p><code>folds</code> : The folds of the data to use (a list generated by the function generateCVRuns TunePareto). If NULL the folds are created internally with the same function. <em>Here</em> we will let the algorithm run alone the internal function, by choosing <strong>NULL</strong>.</p>

<p><code>alphas</code> : A vector of SES thresholds hyper parameters to be used in CV. <em>Here</em> choose the values 0.1, 0.05, 0.01.</p>

<p><code>max_ks</code> : A vector of SES max_ks hyper parameters to be used in CV. <em>Here</em> we choose the values 3, 4, 5. </p>

<p><code>task</code> : A character (&ldquo;C&rdquo;, &ldquo;R&rdquo; or &ldquo;S&rdquo;). It can be &ldquo;C&rdquo; for classification (logistic, multinomial or ordinal regression), &ldquo;R&rdquo; for regression (robust and non robust linear regression, median regression, (zero inflated) poisson and negative binomial regression, beta regression), &ldquo;S&rdquo; for survival regression (Cox, Weibull or exponential regression). <em>Here</em> we choose &ldquo;C&rdquo;, since we are applying a classification technique on the dataset, by placing as target the &ldquo;Type&rdquo; categorical variable.</p>

<p><code>metric</code> : A metric function provided by the user. If NULL the following functions will be used: auc.mxm, mse.mxm, ci.mxm for classification, regression and survival analysis tasks, respectively. The metric functions that are currently supported are:</p>

<ul>
<li><p>auc.mxm: &ldquo;area under the receiver operator characteristic curve&rdquo; metric, as provided in the package ROCR.</p></li>
<li><p>acc.mxm: accuracy metric.</p></li>
<li><p>fscore.mxm: F score for binary logistic regression.</p></li>
<li><p>euclid_sens.spec.mxm: Euclidean norm of 1 - sensitivity and 1 - specificity for binary logistic regression.</p></li>
<li><p>acc_multinom.mxm: accuracy or multinomial logistic regression.</p></li>
<li><p>mse.mxm: -1 * (mean squared error), for robust and non robust linear regression and median (quantile) regression.</p></li>
<li><p>ci.mxm: 1 - concordance index as provided in the rcorr.cens function from the Hmisc package. This is to be used with the Cox proportional hazards model only.</p></li>
<li><p>ciwr.mxm concordance index as provided in the rcorr.cens function from the Hmisc package. This is to be used with the Weibull regression model only.</p></li>
<li><p>poisdev.mxm: Poisson regression deviance.</p></li>
<li><p>nbdev.mxm: Negative binomial regression deviance.</p></li>
<li><p>binomdev.mxm: Negative binomial regression deviance.</p></li>
<li><p>ord_mae.mxm: Ordinal regression mean absolute error.</p></li>
</ul>

<p>If you are certain about the meaning of one metric, then we suggest to use this one. <em>Here</em> we choose the &ldquo;accuracy metric for multinomial&rdquo; (acc_multinom.mxm).  </p>

<p><code>modeler</code> : A modeling function provided by the user. If NULL the following functions will be used: glm.mxm, lm.mxm, coxph.mxm for classification, regression and survival analysis tasks, respectively. The modelling functions that are currently supported are:</p>

<ul>
<li><p>glm.mxm: fits a glm for a binomial family (Classification task).</p></li>
<li><p>lm.mxm: fits a linear model model (stats) for the regression task.</p></li>
<li><p>coxph.mxm: fits a cox proportional hazards regression model for the survival task.</p></li>
<li><p>weibreg.mxm: fits a Weibull regression model for the survival task.</p></li>
<li><p>rq.mxm: fits a quantile (median) regression model for the regression task.</p></li>
<li><p>lmrob.mxm: fits a robust linear model model for the regression task.</p></li>
<li><p>pois.mxm: fits a poisson regression model model for the regression task.</p></li>
<li><p>nb.mxm: fits a negative binomial regression model model for the regression task.</p></li>
<li><p>multinom.mxm: fits a multinomial regression model model for the regression task.</p></li>
<li><p>ordinal.mxm: fits an ordinal regression model model for the regression task.</p></li>
<li><p>beta.mxm: fits a beta regression model model for the regression task. </p></li>
</ul>

<p>The predicted values are transformed into R using the logit transformation. This is so that the &ldquo;mse.mxm&rdquo; metric function can be used. In addition, this way the performance can be compared with the regression scenario, where the logit is applied and then a regression model is employed.  </p>

<p>In case you decide not to choose one of the default modelers, we suggest to you to be certain about the way your chosen one works. <em>Here</em> we choose multinom.mxm, since we search for a multinomial regression model.  </p>

<p><code>ses_test</code> : A function object that defines the conditional independence test used in the SES function (see <strong>3.1.1 Selecting Appropriate Conditional Independence Test</strong>). If NULL, &ldquo;testIndFisher&rdquo;, &ldquo;testIndLogistic&rdquo; and &ldquo;censIndCR&rdquo; are used for classification, regression and survival analysis tasks, respectively. Not all tests can be included here. &ldquo;testIndClogit&rdquo;, &ldquo;testIndMVreg&rdquo;, &ldquo;testIndIG&rdquo;, &ldquo;testIndGamma&rdquo;, &ldquo;testIndZIP&rdquo; and &ldquo;testIndTobit&rdquo; are not available yet.     </p>

<p><em>Here</em> we choose &#39;testIndMultinom&#39;&#39;, as explained in <strong>4.1.1 Selecting Appropriate Conditional Independence Test</strong>  </p>

<p><code>robust</code> : A boolean variable which indicates whether (TRUE) or not (FALSE) to use a robust version of the statistical test if it is available. It takes more time than a non robust version but it is suggested in case of outliers. <em>Here</em>, we set it as <strong>FALSE</strong>.  </p>

<p><code>ncores</code> :  How many cores to use. For more details, see the <strong>TIPS</strong> session. <em>Here</em>, since we have a small number of samples and features, we are using only one core.</p>

<pre><code class="r">### ~ ~ ~ Cross-Validation ~ ~ ~ ### 
library(&#39;MXM&#39;)
cv_ses_model &lt;- MXM::cv.ses(target   = wine[, 1], # Using the 1st column as target
                            dataset  = wine[, -1],  
                            wei      = NULL,
                            kfolds   = 5, 
                            folds    = NULL, 
                            alphas   = c(0.1, 0.05, 0.01), 
                            max_ks   = c(3, 4, 5), 
                            task     = &quot;C&quot;, 
                            metric   = acc_multinom.mxm, # Note that we are passing it as a function and not as a character
                            modeler  = multinom.mxm,  # Note that we are passing it as a function and not as a character
                            ses_test = &quot;testIndMultinom&quot;,
                            ncores = 1)
</code></pre>

<h2>Output</h2>

<p>The main purpose of running Cross - Validation SES algorithm is to see which hyperparameters should be selected for tuning the algorithm. The best configuration (pair of max_k and p-value) is stored under the variable <code>$best_configuration</code>.</p>

<pre><code class="r">cv_ses_model$best_configuration
</code></pre>

<pre><code>## $id
## [1] 4
## 
## $a
## [1] 0.05
## 
## $max_k
## [1] 5
</code></pre>

<pre><code class="r">cv_ses_model$best_performance
</code></pre>

<pre><code>## [1] 0.9493651
</code></pre>

<p>As we see, the id, the significance threshold (&ldquo;a&rdquo;) and the best max_k are returned. </p>

<p>But what if we want to see how good a specific configuration did during the Cross-Validation?<br/>
Yes yes, we can ask for the performance more specifically.<br/>
Here we check separately the results for the first configuration.</p>

<pre><code class="r">cv_ses_model$cv_results_all[[1]]$configuration #this configuration we are examining
</code></pre>

<pre><code>## $id
## [1] 1
## 
## $a
## [1] 0.1
## 
## $max_k
## [1] 5
</code></pre>

<pre><code class="r">cv_ses_model$cv_results_all[[1]]$performances # those are the performances.
</code></pre>

<pre><code>## [1] 0.9722222 0.8333333 0.9166667 0.9714286 0.9142857
</code></pre>

<pre><code class="r">cv_ses_model$cv_results_all[[1]]$signatures # signatures created by this configuration
</code></pre>

<pre><code>## [[1]]
##      Var1 Var2 Var3 Var4
## [1,]    1    7   11   13
## 
## [[2]]
##      Var1 Var2 Var3 Var4
## [1,]    7   10   12   13
## [2,]    3   10   12   13
## [3,]    7   10   11   13
## [4,]    3   10   11   13
## 
## [[3]]
##      Var1 Var2 Var3 Var4
## [1,]    1    7   10   13
## [2,]    1    7    4   13
## [3,]    1    7   11   13
## 
## [[4]]
##      Var1 Var2 Var3 Var4
## [1,]    1    7   11   13
## 
## [[5]]
##      Var1 Var2 Var3 Var4
## [1,]    1    7   10   13
</code></pre>

<p><strong>Note:</strong> By asking the best performance <code>$best_performance</code>, we get the mean value of all folds of the best configuration. If we ask for the performances of the best configuration (here it is the first configuration) <code>$cv_results_all[[1]]$performances</code> we get all performance values of each fold (here 5). </p>

<pre><code class="r">cv_ses_model$best_performance
</code></pre>

<pre><code>## [1] 0.9493651
</code></pre>

<pre><code class="r">index&lt;-cv_ses_model$best_configuration$id
cv_ses_model$cv_results_all[[index]]$performances
</code></pre>

<pre><code>## [1] 0.9722222 0.9722222 0.9166667 0.9714286 0.9142857
</code></pre>

<pre><code class="r">mean(cv_ses_model$cv_results_all[[index]]$performances)  
</code></pre>

<pre><code>## [1] 0.9493651
</code></pre>

<h1>Tips</h1>

<h2>Choosing k_max</h2>

<p>There is a rule of thumb that we are suggesting. If the sample size is small, take as max_k the smallest integer near N/10, where N is the number of samples. For more details, you may visit <a href="http://bit.ly/stack_ovrflw_max_predictors_in_multi_regression">http://bit.ly/stack_ovrflw_max_predictors_in_multi_regression</a></p>

<pre><code class="r">#
N = dim(wine_dataset)[1]
suggested_max_k = floor(N/10)
N
</code></pre>

<pre><code>## [1] 178
</code></pre>

<pre><code class="r">suggested_max_k
</code></pre>

<pre><code>## [1] 17
</code></pre>

<p>However, an other approach would be to run the cross-validation function, described above. In that case, the best <code>max_k</code> will be returned together with the best <code>threshold</code> (see <strong>6. Cross - Validation</strong>)</p>

<h2>Choosing number of cores</h2>

<p>This those kind of analyses may be applied on very big dataset, the SES algorithm provides the opportunity to be run on more than one cores. We suggest that you use more cores, when the datasets are big, but remember that the best run time will always be dependent on the architecture of the computer that is used.   </p>

<h1>Conclusion</h1>

<blockquote>
<p>Now you are ready to run your own analysis using MXM::SES algorithm!<br/>
Thank you for your attention.<br/>
Hope that you found this tutorial helpful.    </p>
</blockquote>

<h1>Session Info {.unnumbered}</h1>

<p>All analyses have been applied on:</p>

<pre><code class="r">sessionInfo()
</code></pre>

<pre><code>## R version 3.6.0 (2019-04-26)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 17134)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=C                  LC_CTYPE=Greek_Greece.1253   
## [3] LC_MONETARY=Greek_Greece.1253 LC_NUMERIC=C                 
## [5] LC_TIME=Greek_Greece.1253    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] dplyr_0.8.1 MXM_1.4.4  
## 
## loaded via a namespace (and not attached):
##  [1] Rfast2_0.0.2        tidyselect_0.2.5    xfun_0.7           
##  [4] slam_0.1-45         sets_1.0-18         purrr_0.3.2        
##  [7] splines_3.6.0       lattice_0.20-38     htmltools_0.3.6    
## [10] survival_2.44-1.1   rlang_0.3.4         R.oo_1.22.0        
## [13] nloptr_1.2.1        pillar_1.4.0        glue_1.3.1         
## [16] R.utils_2.8.0       RcppZiggurat_0.1.5  foreach_1.4.4      
## [19] R.cache_0.13.0      stringr_1.4.0       MatrixModels_0.4-1 
## [22] bdsmatrix_1.3-3     R.methodsS3_1.7.1   visNetwork_2.0.6   
## [25] htmlwidgets_1.3     codetools_0.2-16    evaluate_0.13      
## [28] geepack_1.2-1       coxme_2.2-10        knitr_1.22         
## [31] SparseM_1.77        doParallel_1.0.14   parallel_3.6.0     
## [34] quantreg_5.38       markdown_0.9        Rfast_1.9.4        
## [37] Rcpp_1.0.1          relations_0.6-8     jsonlite_1.6       
## [40] R.rsp_0.43.1        lme4_1.1-21         digest_0.6.18      
## [43] stringi_1.4.3       ordinal_2019.4-25   numDeriv_2016.8-1  
## [46] grid_3.6.0          tools_3.6.0         magrittr_1.5       
## [49] tibble_2.1.1        cluster_2.0.8       ucminf_1.1-4       
## [52] bigmemory.sri_0.1.3 bigmemory_4.5.33    crayon_1.3.4       
## [55] pkgconfig_2.0.2     MASS_7.3-51.4       Matrix_1.2-17      
## [58] energy_1.7-5        iterators_1.0.10    assertthat_0.2.1   
## [61] minqa_1.2.4         R6_2.4.0            boot_1.3-22        
## [64] nnet_7.3-12         nlme_3.1-139        compiler_3.6.0
</code></pre>

<h1>References {.unnumbered}</h1>

<p>Lagani, V., Athineou, G., Farcomeni, A., Tsagris, M. &amp; Tsamardinos, I. Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. J. Stat. Softw. 80, 1-25 (2017).  </p>

<p>I. Tsamardinos, V. Lagani and D. Pappas (2012). Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.  </p>

<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.  </p>

<p>Brown, L. E., Tsamardinos, I., &amp; Aliferis, C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. Medinfo, 711-715.  </p>

<p>Tsamardinos, I., Aliferis, C. F., &amp; Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.  </p>

</body>

</html>
