Automatically generated by Mendeley Desktop 1.13.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Tsamardinos2014,
author = {Tsamardinos, Ioannis and Lagani, Vincenzo and Rakhshani, Amin},
booktitle = {SETN'14 Proceedings of the 79h Hellenic conference on Artificial Intelligence},
title = {{Performance-Estimation Properties of Cross-Validation-Based Protocols with Simultaneous Hyper-Parameter Optimization}},
year = {2014}
}
@book{Neapolitan2003,
abstract = {In this first edition book, methods are discussed for doing inference in Bayesian networks and inference diagrams. Hundreds of examples and problems allow readers to grasp the information. Some of the topics discussed include Pearl's message passing algorithm, Parameter Learning: 2 Alternatives, Parameter Learning r Alternatives, Bayesian Structure Learning, and Constraint-Based Learning. For expert systems developers and decision theorists.},
author = {Neapolitan, Richard E},
booktitle = {Molecular Biology},
isbn = {0130125342},
issn = {17485673},
pages = {674},
title = {{Learning Bayesian Networks}},
url = {http://www.amazon.com/Learning-Bayesian-Networks-Richard-Neapolitan/dp/0130125342},
volume = {6},
year = {2003}
}
@article{Aliferis2003a,
abstract = {Causal Probabilistic Networks (CPNs), (a.k.a. Bayesian Networks, or Belief Networks) are well-established representations in biomedical applications such as decision support systems and predictive modeling or mining of causal hypotheses. CPNs (a) have well-developed theory for induction of causal relationships, and (b) are suitable for creating sound and practical decision support systems. While several public domain and commercial tools exist for modeling and inference with CPNs, very few software tools and libraries exist currently that give access to algorithms for CPN induction. To that end, we have developed a software library, called Causal Explorer, that implements a suit of global, local and partial CPN induction algorithms. The toolkit emphasizes causal discovery algorithms. Approximately half of the algorithms are enhanced implementations of well-established algorithms, and the remaining ones are novel local and partial algorithms that scale to thousands of variables and thus are particularly suitable for modeling in massive datasets.},
author = {Aliferis, C F and Statnikov, A R and Tsamardinos, I and Brown, L E},
isbn = {1932415041},
journal = {The 2003 International Conference on Mathematics and Engineering Techniques in Medicine and Biological Sciences (METMBS '03)},
keywords = {1 introduction and goals,bayesian models in medicine,community,data mining and bioinformatics,software tools for bioinformatics},
title = {{Causal Explorer : A Causal Probabilistic Network Learning Toolkit for Biomedical Discovery}},
year = {2003}
}
@misc{Landsheer2010,
abstract = {Tetrad IV is a program designed for the specification of causal models. It is specifically designed to search for causal relations, but also offers the possibility to estimate the parameters of a structural equation model. It offers a remarkable graphical user interface, which facilitates building, evaluating, and searching for causal models. The search algorithms make it possible to find alternatives for existing models, as well as to find new models when a theoretical directive is lacking. This is illustrated by the detection of a causal model for longitudinal data, which is a viable alternative for a latent growth model.},
author = {Landsheer, J. A.},
booktitle = {Structural Equation Modeling: A Multidisciplinary Journal},
doi = {10.1080/10705511.2010.510074},
issn = {1070-5511},
pages = {703--711},
title = {{The Specification of Causal Models with Tetrad IV: A Review}},
volume = {17},
year = {2010}
}
@article{Statnikov2010a,
abstract = {Molecular signatures are computational or mathematical models created to diagnose disease and other phenotypes and to predict clinical outcomes and response to treatment. It is widely recognized that molecular signatures constitute one of the most important translational and basic science developments enabled by recent high-throughput molecular assays. A perplexing phenomenon that characterizes high-throughput data analysis is the ubiquitous multiplicity of molecular signatures. Multiplicity is a special form of data analysis instability in which different analysis methods used on the same data, or different samples from the same population lead to different but apparently maximally predictive signatures. This phenomenon has far-reaching implications for biological discovery and development of next generation patient diagnostics and personalized treatments. Currently the causes and interpretation of signature multiplicity are unknown, and several, often contradictory, conjectures have been made to explain it. We present a formal characterization of signature multiplicity and a new efficient algorithm that offers theoretical guarantees for extracting the set of maximally predictive and non-redundant signatures independent of distribution. The new algorithm identifies exactly the set of optimal signatures in controlled experiments and yields signatures with significantly better predictivity and reproducibility than previous algorithms in human microarray gene expression datasets. Our results shed light on the causes of signature multiplicity, provide computational tools for studying it empirically and introduce a framework for in silico bioequivalence of this important new class of diagnostic and personalized medicine modalities.},
author = {Statnikov, Alexander and Aliferis, Constantin F},
doi = {10.1371/journal.pcbi.1000790},
file = {:C$\backslash$:/Users/Vincenzo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Statnikov, Aliferis - 2010 - Analysis and computational dissection of molecular signature multiplicity.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Area Under Curve,Computational Biology,Computational Biology: methods,Computer Simulation,Databases, Genetic,Gene Expression Profiling,Humans,Markov Chains,Reproducibility of Results},
month = may,
number = {5},
pages = {e1000790},
pmid = {20502670},
title = {{Analysis and computational dissection of molecular signature multiplicity.}},
url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000790},
volume = {6},
year = {2010}
}
@article{Tsamardinos2006a,
abstract = {Abstract We present a new algorithm for Bayesian network structure learning , called Max - Min Hill - Climbing (MMHC). The algorithm combines ideas from local learning , constraint- based, and search-and-score techniques in a principled and effective way. It first ... $\backslash$n},
author = {Tsamardinos, Ioannis and Brown, Laura E. and Aliferis, Constantin F.},
doi = {10.1007/s10994-006-6889-7},
issn = {0885-6125},
journal = {Machine Learning},
number = {1},
pages = {31--78},
title = {{The max-min hill-climbing Bayesian network structure learning algorithm}},
volume = {65},
year = {2006}
}
@conference{Tsamardinos12,
address = {Heraklion,},
author = {Tsamardinos, Ioannis and Lagani, Vincenzo and Pappas, Dimitris},
booktitle = {7th Conference of the Hellenic Society for Computational Biology and Bioinformatics (HSCBB12)},
organization = {HSCBB2012},
title = {{Discovering multiple, equivalent biomarker signatures}},
year = {2012}
}
@article{Scutari2010,
abstract = {bnlearn is an R package which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package.},
archivePrefix = {arXiv},
arxivId = {0908.3817},
author = {Scutari, Marco},
eprint = {0908.3817},
file = {:C$\backslash$:/Users/Vincenzo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scutari - 2010 - Learning Bayesian Networks with the bnlearn R Package.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {R,bayesian networks,conditional independence tests,constraint-based algorithms,score-based algorithms,structure learning algorithms},
pages = {1--22},
title = {{Learning Bayesian Networks with the bnlearn R Package}},
url = {http://arxiv.org/abs/0908.3817},
volume = {35},
year = {2010}
}
@article{Aliferis2010a,
abstract = {We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distributions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning.},
author = {Aliferis, Constantin F},
file = {:C$\backslash$:/Users/Vincenzo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aliferis - 2010 - Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I Algori.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {171--234},
pmid = {1181001},
title = {{Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I : Algorithms and Empirical Evaluation}},
volume = {11},
year = {2010}
}
@book{Spirtes1993,
address = {New York, NY},
author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
doi = {10.1007/978-1-4612-2748-9},
isbn = {978-1-4612-7650-0},
publisher = {Springer New York},
series = {Lecture Notes in Statistics},
title = {{Causation, Prediction, and Search}},
url = {http://www.springerlink.com/index/10.1007/978-1-4612-2748-9},
volume = {81},
year = {1993}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
archivePrefix = {arXiv},
arxivId = {NIHMS201118},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.1359/JBMR.0301229},
eprint = {NIHMS201118},
file = {:C$\backslash$:/Users/Vincenzo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2010 - Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
isbn = {096368910X},
issn = {1548-7660},
journal = {Journal of statistical software},
pages = {1--22},
pmid = {20808728},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2929880\&tool=pmcentrez\&rendertype=abstract},
volume = {33},
year = {2010}
}
@article{Lagani2010b,
abstract = {MOTIVATION: Variable selection is a typical approach used for molecular-signature and biomarker discovery; however, its application to survival data is often complicated by censored samples. We propose a new algorithm for variable selection suitable for the analysis of high-dimensional, right-censored data called Survival Max-Min Parents and Children (SMMPC). The algorithm is conceptually simple, scalable, based on the theory of Bayesian networks (BNs) and the Markov blanket and extends the corresponding algorithm (MMPC) for classification tasks. The selected variables have a structural interpretation: if T is the survival time (in general the time-to-event), SMMPC returns the variables adjacent to T in the BN representing the data distribution. The selected variables also have a causal interpretation that we discuss. RESULTS: We conduct an extensive empirical analysis of prototypical and state-of-the-art variable selection algorithms for survival data that are applicable to high-dimensional biological data. SMMPC selects on average the smallest variable subsets (less than a dozen per dataset), while statistically significantly outperforming all of the methods in the study returning a manageable number of genes that could be inspected by a human expert. AVAILABILITY: Matlab and R code are freely available from http://www.mensxmachina.org},
author = {Lagani, Vincenzo and Tsamardinos, Ioannis},
institution = {Institute of Computer Science, Foundation for Research and Technology-Hellas (FORTH) and Computer Science Department, University of Crete, Heraklion, Greece. vlagani@ics.forth.gr},
journal = {Bioinformatics},
number = {15},
pages = {1887--1894},
pmid = {20519286},
title = {{Structure-based variable selection for survival data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20519286},
volume = {26},
year = {2010}
}
@article{Guyon2003,
abstract = {Abstract Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods. Keywords: Variable selection, feature selection, space dimensionality reduction, pattern discovery, filters, wrappers, clustering, information theory, support vector machines, model selection, statistical testing, bioinformatics, computational biology, gene expression, microarray, genomics, proteomics, QSAR, text classification, information retrieval.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, I and Guyon, I and Elisseeff, A and Elisseeff, A},
doi = {10.1162/153244303322753616},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/Vincenzo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guyon et al. - 2003 - An introduction to variable and feature selection.pdf:pdf},
isbn = {1877263877},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {1157--1182},
pmid = {294587216266485240},
title = {{An introduction to variable and feature selection}},
volume = {3},
year = {2003}
}
@inproceedings{Tsamardinos2003c,
abstract = {In an influencial paper Kohavi and John [7] presented a number of disadvantages of the filter approach to the feature selection problem, steering research towards algorithms adopting the wrapper approach. We show here that neither approach is inherently better and that any practical feature selection algorithm needs to at least consider the learner used for classification and the metric used for evaluating the learner's performance. In the process we formally define the feature selection problem, re-examine the relationship between relevancy and filter algorithms, and establish a connection between Kohavi and John's definition of relevancy to the Markov Blanket of a target variable in a Bayesian Network faithful to some data distribution.},
author = {Tsamardinos, Ioannis and Aliferis, Constantin F.},
booktitle = {Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics},
issn = {<null>},
title = {{Towards principled feature selection: relevancy, filters, and wrappers}},
year = {2003}
}
@inproceedings{Huang2015,
author = {Huang, GraceT and Tsamardinos, Ioannis and Raghu, Vineet and Kaminski, Naftali and Benos, Panayiotis V},
booktitle = {Pacific Symposium on Biocomputing (PSB)},
title = {{T-Recs: Stable selection of Dynamically Formed Groups of Features with Application to Prediction of Clinical Outcomes}},
year = {2015}
}

@article{gutenbrunner1993,
  title={Tests of linear hypotheses based on regression rank scores},
  author={Gutenbrunner, C and Jure{\v{c}}kov{\'a}, J and Koenker, R and Portnoy, S},
  journal={Journal of Nonparametric Statistics},
  volume={2},
  number={4},
  pages={307--331},
  year={1993},
  publisher={Taylor \& Francis}
}

@book{koenker2005,
  title={Quantile regression},
  author={Koenker, Roger},
  year={2005},
  publisher={Cambridge university press}
}

@book{maronna2006,
  title={Robust statistics},
  author={Maronna, RARD and Martin, Douglas and Yohai, Victor},
  year={2006},
  publisher={John Wiley \& Sons, Chichester. ISBN}
}

@Manual{rstudio_ref,
    title = {RStudio: Integrated Development Environment for R},
    author = {{RStudio Team}},
    organization = {RStudio, Inc.},
    address = {Boston, MA},
    year = {2015},
    url = {http://www.rstudio.com/},
  }


 